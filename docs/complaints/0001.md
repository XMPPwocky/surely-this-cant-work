# 0001: Kernel Stack Frames Are Too Large

## Status: Open

## Summary

Several kernel functions have stack frames in the 8-14 KiB range. The worst
call chain (init spawning a user ELF from the filesystem) totals 37.3 KiB,
which overflowed the original 32 KiB kernel stacks and required bumping to
64 KiB + guard pages. This is wasteful: 64 processes * 68 KiB (64K stack +
4K guard) = 4.25 MiB just for kernel stacks in a 128 MiB system.

## The Offending Call Chain

Measured from frame pointer differences during a stack overflow backtrace:

| Function | Frame Size | Why |
|---|---|---|
| `init_server` | ~7 KiB* | `dyn_spawns`, `endpoints`, `Message` locals |
| `finish_fs_launch` | 5.8 KiB | Multiple `Message` structs (1 KiB each) for IPC |
| `spawn_user_elf_with_boot_channel` | 9.2 KiB | `Process` struct constructed on stack, then moved |
| `new_user_elf` | 8.4 KiB | `Process` struct (~11 KiB) built as local variable |
| **Total** | **~30 KiB** | |

*Was 14 KiB before `FsLaunchCtx` array was heap-allocated via `Box::new_in`.

## Root Causes

### 1. `Process` struct is ~11 KiB

The `Process` struct contains:
- `mmap_regions: [Option<MmapRegion>; 256]` -- 256 * 20 bytes = 5 KiB
- `handles: [Option<HandleObject>; 32]` -- 32 * 24 bytes = 768 bytes
- `pt_frames: Vec<PhysPageNum>` -- 24 bytes (but heap-backed)
- `context: TaskContext` -- 112 bytes
- Various scalars, name buffer, EWMA fields -- ~200 bytes

Every call to `new_user_elf` or `new_user` constructs this on the stack,
then moves it into `sched.processes[pid] = Some(proc)`. The move is a
memcpy, but the 11 KiB temporary still lives on the caller's stack frame
for the duration of the function.

### 2. `Message` struct is 1 KiB

`Message` contains a `data: [u8; 1024]` buffer. Every function that builds
or receives an IPC message has at least one of these on its stack. Functions
like `finish_fs_launch` that make multiple IPC calls can have 3-4 of them
simultaneously, consuming 3-4 KiB of stack per function.

### 3. Call chain depth

The init -> finish_fs_launch -> spawn_user_elf -> new_user_elf chain is 4
functions deep, each with a large frame. The frames compound because none
of them are tail calls.

## Possible Fixes

### Box the Process struct (biggest win)

Change `spawn_user_elf_*` to allocate `Box<Process>` on the heap and store
`Box<Process>` (or write directly into the scheduler's slot) instead of
building a local and moving it. This would save ~11 KiB from every spawn
call's stack frame.

The scheduler's `processes: Vec<Option<Process>>` could become
`Vec<Option<Box<Process>>>`, or the `new_*` constructors could take a `&mut`
reference to an already-allocated slot.

### Pass Message by reference or reuse buffers

Instead of each function declaring its own `let mut msg = Message::new()`,
pass a `&mut Message` scratch buffer down the call chain or use a
per-task message buffer. This would collapse multiple 1 KiB frames into one.

### Flatten the spawn call chain

`spawn_user_elf_with_boot_channel` and `spawn_user_elf_with_handles` are
thin wrappers that call `new_user_elf` and then set handles. These could be
inlined or combined so the `Process` temporary only lives in one frame.

### Reduce MAX_MMAP_REGIONS

256 mmap regions per process is generous. Reducing to 64 would save
(256-64) * 20 = 3.8 KiB per `Process`, bringing the struct from ~11 KiB
down to ~7 KiB. But this limits how many mmap calls a process can make.

### Indirect the mmap array

Store mmap regions in a heap-allocated `Vec` instead of an inline array.
This makes `Process` small (just a pointer + len + cap = 24 bytes instead
of 5 KiB) at the cost of an extra heap allocation per process.

## Current Mitigation

- Kernel stacks are 64 KiB (16 pages) + 1 guard page = 68 KiB per process
- Guard page catches overflow with a clean page fault + backtrace
- `FsLaunchCtx` array in init_server heap-allocated via `Box::new_in`
- Total kernel stack memory: ~4.25 MiB for 64 process slots
